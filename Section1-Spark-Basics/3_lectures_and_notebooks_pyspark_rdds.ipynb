{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark notebook basics\n",
    "\n",
    "**SparkContext**: our way of comunicating to the Spark system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:30.650946Z",
     "start_time": "2018-08-10T20:43:30.636873Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:34.758495Z",
     "start_time": "2018-08-10T20:43:30.654510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master='local[4]')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `master='local[4]` runs spark locally in my notebook using 4 workers (since I have 4 cores, we have one worker per core)\n",
    " \n",
    " We must have only one SparkContext at a time. It is designed for a single user. Before running a new SparkContext, stop the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:34.783716Z",
     "start_time": "2018-08-10T20:43:34.770261Z"
    }
   },
   "outputs": [],
   "source": [
    "# sc.stop() # stop current SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDDs** (Resilient Distributed Dataset): a list of elements stored in several computers\n",
    "\n",
    "**Parallelize**: simplest wat of creating an RDD. It is of type `PythonRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:35.349519Z",
     "start_time": "2018-08-10T20:43:34.792475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize(range(3))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect**: RDD content is distributed among all executors. `collect()` is the inverse of `parallelize()`. Collects the elements of the RDD, returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:36.612860Z",
     "start_time": "2018-08-10T20:43:35.352552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "L=A.collect()\n",
    "print(type(L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using collect eleminates the benefits of parallelism!!\n",
    "\n",
    "It is often tempting to `.collect()` an RDD to make it into a list, and then process it using standard Python. However, this means only the head node is performing the computation, thus not benefitting from Spark. Using RDD operations will make you use all the computers at your disposal\n",
    "\n",
    "**Map**: applies an operation to each element of the RDD. Parameter is the function defining the operation. Returns a new RDD. Operation is performed in parallel on all execution. Each executor operates on the local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:36.744478Z",
     "start_time": "2018-08-10T20:43:36.616403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce**: takes an RDD, return a single value. Reduce operator takes two elements as input and returns one as output. Repeatedly applies a reduce operator. Each executor reduces the data local to it. The results from all executors are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:36.857823Z",
     "start_time": "2018-08-10T20:43:36.747869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:37.009336Z",
     "start_time": "2018-08-10T20:43:36.861214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finds the shortest string\n",
    "words=['this','is','the','best','thinkpad','ever!']\n",
    "wordsRDD=sc.parallelize(words)\n",
    "wordsRDD.reduce(lambda w,v: w if len(w)<len(v) else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:37.169481Z",
     "start_time": "2018-08-10T20:43:37.011589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad reduce operation\n",
    "B=sc.parallelize([1,3,5,2])\n",
    "B.reduce(lambda x,y: x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't an operation in which the order doesn't matter, because $x-y$ is different from $y-x$. Which of the following did you do:\n",
    "\n",
    "$$((1-3)-5)-2$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(1-3)-(5-2)$$\n",
    "\n",
    "\n",
    "Using regular functions instead of lambda functions: lambda functions are short and sweet, but sometimes it is hard to use it in one line, we can use a full-fledged functions instead\n",
    "\n",
    "Suppose we want to find the last word in a lexigographical order among the longest words in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:37.187095Z",
     "start_time": "2018-08-10T20:43:37.172324Z"
    }
   },
   "outputs": [],
   "source": [
    "def largerThan(x,y):\n",
    "    if len(x)>len(y):\n",
    "        return x\n",
    "    elif len(y)>len(x):\n",
    "        return y\n",
    "    else:\n",
    "        if x>y:\n",
    "            return x\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:43:37.345549Z",
     "start_time": "2018-08-10T20:43:37.190156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thinkpad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing number of workers\n",
    "\n",
    "**Effect of changing the number of workers**: when you initialize SparkContext, you can specify the number of workers, usually one worker per core, but it can be smaller or larger than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.452068Z",
     "start_time": "2018-08-10T20:43:37.349467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 executors, time=2.357\n",
      " 2 executors, time=1.957\n",
      " 3 executors, time=2.202\n",
      " 4 executors, time=2.323\n",
      " 5 executors, time=1.890\n",
      " 6 executors, time=2.287\n",
      " 7 executors, time=2.358\n",
      " 8 executors, time=2.808\n",
      " 9 executors, time=2.976\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc.stop()\n",
    "for j in range(1,10):\n",
    "    sc = SparkContext(master='local[%d]'%(j))\n",
    "    tic=time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2]*100000).reduce(lambda x,y:x+y)\n",
    "    print('%2d executors, time=%4.3f'%(j,time()-tic))\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution plans lazy eval and caching\n",
    "\n",
    "Suppose our task is $\\sum^n_{i=1}x^2_i$. The standard (busy) way to do this is: i) calculate the square of each element; ii) sum the squares. This requires storing all intermediate results!\n",
    "\n",
    "**Lazy evaluation**: postpone computing the square until result is needed, no need to intermediate results, scan through the data once, rather than twice. We create an RDD with one million elements to amplify the effects of lazy evaluation and caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.607798Z",
     "start_time": "2018-08-10T20:44:09.455096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 0 ns, total: 17.6 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc=SparkContext()\n",
    "RDD=sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a computation: the role of the function `taketime` is to consume CPU cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.623683Z",
     "start_time": "2018-08-10T20:44:09.615331Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import cos\n",
    "def taketime(i):\n",
    "    [cos(j) for j in range(100)]\n",
    "    return cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.646804Z",
     "start_time": "2018-08-10T20:44:09.631788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 µs, sys: 10 µs, total: 140 µs\n",
      "Wall time: 148 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5403023058681398"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taketime(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time units\n",
    "* 1 second = 1000 millisecond ($ms$)\n",
    "* 1 millisecond = 1000 microsecond ($\\mu s$)\n",
    "* 1 microsecond = 1000 nanosecond ($\\eta s$)\n",
    "\n",
    "Clock rate: one cycle of a 3GHz CPU takes $\\frac{1}{3} \\eta s$\n",
    "\n",
    "`taketime(1000)` takes about $25\\eta s=75,000$ clock cycles\n",
    "\n",
    "**Map operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.665459Z",
     "start_time": "2018-08-10T20:44:09.653743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 µs, sys: 3 µs, total: 35 µs\n",
      "Wall time: 41.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How come so fast??\n",
    "\n",
    "We expected this map operation to take $1000000\\times 25\\eta s=25 seconds$\n",
    "\n",
    "Why did the previous took ~30 seconds??\n",
    "\n",
    "Because no computation was done! The cell defined an execution plan, but did not execute it yet\n",
    "\n",
    "**Execution plans**: at this point the variable `Interm` doesn't point to an actual data structure. Instead, it points to an execution plan expressed as a **dependence graph**. The dependence graph defines how the RDDs are computed from each other. The dependenec graph associated with an RDD can be printed out using the method `toDebugString()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:09.709169Z",
     "start_time": "2018-08-10T20:44:09.674151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[1] at RDD at PythonRDD.scala:48 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 []\n"
     ]
    }
   ],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interm** (4) PythonRDD[1] at RDD at PythonRDD.scala:48 [], the [4] corresponds to the number of partitions\n",
    "\n",
    "**RDD** ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175\n",
    "\n",
    "So, at this point, only the left blocks of the plan have been declared (the parallelize and the map), but not the reduce\n",
    "\n",
    "**Actual execution**: the `reduce` command needs to output an actual output, Spark, therefore has to actually execute the `map` and the `reduce`. Some real computation needs to be done, which takes about 1-3 seconds depending on the machine used and on it's load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:25.408970Z",
     "start_time": "2018-08-10T20:44:09.715666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 15.6 ms, sys: 4 ms, total: 19.6 ms\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How come so fast?**\n",
    "\n",
    "We expected this map operation to take 25 seconds, map+reduce take only ~10 seconds, why?\n",
    "\n",
    "Because we have 4 workers rather than one, because the measurement of a single iteration of `taketime` is an overestimate\n",
    "\n",
    "**Executing a different calculation based on the same plan**: the plan defined by `Interm` might need to be executed more than once. For example, compute the number of map outputs that are larger than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:38.709781Z",
     "start_time": "2018-08-10T20:44:25.412553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 12.4 ms, sys: 18 µs, total: 12.4 ms\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The price of not materializing**: the runtime is similar of that of the reduce, because the intermediate results in `Interm` have not been saved in memory (materialized), they need to be recomputed\n",
    "\n",
    "The middle block is executed twice, once for each final step\n",
    "\n",
    "    RDD                     Interm\n",
    "parallelize(range(1000000)) -> map(taketime()) -> reduce(lambda x,y: x+y)       -> number\n",
    "\n",
    "                                                             \\\n",
    "                                                             filter(lambda x: x>0).count()   -> number\n",
    "                                                             \n",
    "**Caching intermediate results**: we sometimes want to keep the intermediate results in memory so that we can reuse them later without recalculating. This will reduce the running time, at the cost of requiring more memory.\n",
    "\n",
    "The method `cache()` indicates that the RDD generated in this plan should be stored in memory. Note that this is a **plan to cache**. The actual caching will be done only when the final result is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:38.729614Z",
     "start_time": "2018-08-10T20:44:38.712080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.89 ms, sys: 363 µs, total: 6.25 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan to cache**: the definition of `Interm` is almost the same as before. However, the plan corresponding to `Interm` is more elaborate and contains information about how the intermediate results will be cached nad replicated. Note that `PythonRDD[4]` is now `Memory Serialized 1x Replicated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:38.742719Z",
     "start_time": "2018-08-10T20:44:38.731771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[4] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the cache**: the following command executes the first map-reduce command **and** caches the result of the `map` command in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:50.960887Z",
     "start_time": "2018-08-10T20:44:38.745139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 7.42 ms, sys: 4 ms, total: 11.4 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the cache**: this time `Interm` is cached. Therefore the second use of `Interm` is much faster than when we did'nt use cache: 0.35 seconds instead of 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:51.261701Z",
     "start_time": "2018-08-10T20:44:50.968923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 8.83 ms, sys: 57 µs, total: 8.89 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x: x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition and gloming\n",
    "\n",
    "When you create an RDD, you can specify the number of partitions. The default is the number os workers you defined when you set up the `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:51.279994Z",
     "start_time": "2018-08-10T20:44:51.264986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A=sc.parallelize(range(100000))\n",
    "print(A.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:51.356080Z",
     "start_time": "2018-08-10T20:44:51.283682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# you can repartition an RDD into a different number\n",
    "D=A.repartition(10)\n",
    "print(D.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:51.370920Z",
     "start_time": "2018-08-10T20:44:51.359001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# we can also define the number of partitions when creating the RDD\n",
    "A=sc.parallelize(range(100000), numSlices=10)\n",
    "print(A.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why is the number of partitions important?\n",
    "\n",
    "They define the unit the executor works on. You should have at least as many partitions as you have workers (otherwise you will have idle workers). Sometimes smaller partitions can allow more parallelization\n",
    "\n",
    "**Repartition for load balance**\n",
    "\n",
    "Suppose we start with 10 partitions, all with exactly the same number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:52.149564Z",
     "start_time": "2018-08-10T20:44:51.375582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "A=sc.parallelize(range(100000))\\\n",
    "    .map(lambda x: (x,x)).partitionBy(10)\n",
    "print(A.glom().map(len).collect()) # how many elements are in each part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to use `filter()` to select some of the elements in `A`\n",
    "\n",
    "Some partitions might have more elements remaining than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:52.397663Z",
     "start_time": "2018-08-10T20:44:52.153179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 0, 0, 0, 0, 10000, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# select 10% of the entries\n",
    "B=A.filter(lambda pair: pair[0] % 5 == 0)\n",
    "# get number of partitions\n",
    "print(B.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future operations on `B` will use only two workers. The other workers willdo nothing because their partitions are empty.\n",
    "\n",
    "To fix this situation we need to repartition the RDD. One way to do that is to repartition using a new key\n",
    "\n",
    "The method `.partitionBy(k)` expects to get a `(key,value)` RDD where key are integers. Partitions the RDD into `k` partitions. The element `(key,value)` is placed into partition number `key % k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:53.069402Z",
     "start_time": "2018-08-10T20:44:52.400696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
     ]
    }
   ],
   "source": [
    "C=B.map(lambda pair: (pair[1]/10, pair[1])).partitionBy(10)\n",
    "print(C.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use random partitioning using `repartition(k)`\n",
    "\n",
    "An advantage of random partitioning is that it does not require defining a key. A disadvantage of random partitioning is that you have no control on the partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:53.616608Z",
     "start_time": "2018-08-10T20:44:53.074223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000]\n"
     ]
    }
   ],
   "source": [
    "C=B.repartition(10)\n",
    "print(C.glom().map(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**glom()**\n",
    "\n",
    "In general, Spark does not allow the worker to refer to specific elements of the RDD. Keeps the language clean, but can be a major limitation\n",
    "\n",
    "`glom()` transforms each partition into a tuple (immutable list) of elements. Creates an RDD of tuples, one tuple per partition.\n",
    "\n",
    "workers can refer to elements of the partition index, but you cannot assign values to the elements, the RDD is still immutable\n",
    "\n",
    "Now we can understand the command used above to count the number of elements in each partition.\n",
    "\n",
    "We use `glom()` to make each partition into a tuple\n",
    "\n",
    "We use `len` on each partition to get the length of the tuple (size of the partition)\n",
    "\n",
    "We `collect` the results to print them out\n",
    "\n",
    "A more elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:53.954742Z",
     "start_time": "2018-08-10T20:44:53.627051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 10000, 99990), None, None, None, None, (5, 10000, 99990), None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "def getPartitionInfo(G):\n",
    "    d=0\n",
    "    if len(G) > 1:\n",
    "        for i in range(len(G)-1):\n",
    "            d += abs(G[i+1][1] - G[i][1])\n",
    "        return (G[0][0], len(G), d)\n",
    "    else:\n",
    "        return (None)\n",
    "output=B.glom().map(lambda B: getPartitionInfo(B)).collect()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark basics part II\n",
    "\n",
    "**Chaining**\n",
    "\n",
    "We can chain transformations and actions to create a computation **pipeline**: suppose we want to compute the sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:54.117846Z",
     "start_time": "2018-08-10T20:44:53.959249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequential syntax\n",
    "B=sc.parallelize(range(4))\n",
    "Squares=B.map(lambda x: x*x)\n",
    "Squares.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:54.253027Z",
     "start_time": "2018-08-10T20:44:54.133703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cascaded syntax\n",
    "B.map(lambda x: x*x)\\\n",
    "    .reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both syntaxes mean exactly the same, the only difference is in the sequential syntax the intermediate RDD has a name _Squares_ whereas in the cascaded syntax the intermediate RDD is _anonymous_. The execution is identical\n",
    "\n",
    "**Sequential execution** means perform a map, store the resulting RDD in memory, perform the reduce. The disadvantages are the intermediate result requires memory space and two scans of memory (`B` and `Squares`) double the cache misses\n",
    "\n",
    "**Pipelined execution** perform the whole computation in a single pass. For each element of `B` compute the square and input it to the reduce operation. Advantages are less memory required (intermediate result is not stored) and faster (only one pass through the input RDD)\n",
    "\n",
    "**Lazy evaluation** this pipelined evaluation is called lazy evaluation, lazy because computing the square is not executed immediately, instead the execution is delayed as long as possible so that several commands are executed in a single pass. The delayed commands are organized in an **execution plan**\n",
    "\n",
    "**an instructive mistake**: here is another way to compute the sum of squares with a single reduce command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:54.382910Z",
     "start_time": "2018-08-10T20:44:54.266751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C=sc.parallelize([1,1,2])\n",
    "C.reduce(lambda x,y: x*x+y*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getting information about an RDD**\n",
    "\n",
    "printing the results is not an option..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:54.898406Z",
     "start_time": "2018-08-10T20:44:54.385393Z"
    }
   },
   "outputs": [],
   "source": [
    "n=1000000\n",
    "B=sc.parallelize([1,2,3,4]*int(n/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:55.039047Z",
     "start_time": "2018-08-10T20:44:54.902234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of elements in the RDD\n",
    "\n",
    "B.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:55.136930Z",
     "start_time": "2018-08-10T20:44:55.042812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element 1\n",
      "first 5 elements [1, 2, 3, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "# get first elements of RDD\n",
    "\n",
    "print('first element', B.first())\n",
    "print('first 5 elements', B.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sampling from an RDD**\n",
    "\n",
    "aggregates such as average could be approximated efficiently by using a sample, samlping is done in parallel and requires limited computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:56.433588Z",
     "start_time": "2018-08-10T20:44:55.145997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1 =  [4, 3, 4, 1, 3, 3]\n",
      "sample2 =  [1, 4, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# get a sample of size m\n",
    "# note the size of the sample is different in each run\n",
    "\n",
    "m=5.\n",
    "print('sample1 = ', B.sample(False, m/n).collect())\n",
    "print('sample2 = ', B.sample(False, m/n).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filtering** an RDD: selecting those elements of the source on which a function returns `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:56.806552Z",
     "start_time": "2018-08-10T20:44:56.437939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements in B that are > 3 250000\n"
     ]
    }
   ],
   "source": [
    "print('number of elements in B that are > 3', B.filter(lambda n: n>3).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**removing duplicate elements of an RDD**: the method `RDD.distinct()` returns a new dataset that contains the distinct elements of the source dataset. This operation requires a **shuffle** in order to detectt duplication across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:57.204097Z",
     "start_time": "2018-08-10T20:44:56.810049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuplicateRDD= [1, 1, 2, 2, 3, 3]\n",
      "DistinctRDD= [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "DuplicateRDD=sc.parallelize([1,1,2,2,3,3])\n",
    "print('DuplicateRDD=', DuplicateRDD.collect())\n",
    "print('DistinctRDD=', DuplicateRDD.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap an RDD** is similar to `map`, but each input item can be mapped to 0 or more output items (so a function should return a sequence rather than a single item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:57.448738Z",
     "start_time": "2018-08-10T20:44:57.209613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: [['you', 'are', 'my', 'sunshine'], ['my', 'only', 'sunshine']]\n",
      "flatmap: ['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n"
     ]
    }
   ],
   "source": [
    "text=['you are my sunshine', 'my only sunshine']\n",
    "text_file=sc.parallelize(text)\n",
    "\n",
    "# map each line in text to a list of words\n",
    "print('map:', text_file.map(lambda line: line.split(' ')).collect())\n",
    "\n",
    "# create a single list of words by combining the words from all\n",
    "print('flatmap:', text_file.flatMap(lambda line: line.split(' ')).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**set operation**: union, subtract, cartesian in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:57.484395Z",
     "start_time": "2018-08-10T20:44:57.462359Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize([1,1,2,3])\n",
    "rdd2=sc.parallelize([1,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:58.221528Z",
     "start_time": "2018-08-10T20:44:57.492609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1: [1, 1, 2, 3]\n",
      "rdd2: ['a', 'b', 1]\n",
      "union as bags: [1, 1, 2, 3, 'a', 'b', 1]\n",
      "union as sets: [1, 'a', 2, 3, 'b']\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "rdd2=sc.parallelize(['a','b',1])\n",
    "print('rdd1:', rdd1.collect())\n",
    "print('rdd2:', rdd2.collect())\n",
    "print('union as bags:', rdd1.union(rdd2).collect())\n",
    "print('union as sets:', rdd1.union(rdd2).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:58.866649Z",
     "start_time": "2018-08-10T20:44:58.225111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1: [1, 1, 2, 3]\n",
      "rdd2: [1, 1, 2, 5]\n",
      "intersection: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# intersection\n",
    "rdd2=sc.parallelize([1,1,2,5])\n",
    "print('rdd1:', rdd1.collect())\n",
    "print('rdd2:', rdd2.collect())\n",
    "print('intersection:', rdd1.intersection(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:59.395984Z",
     "start_time": "2018-08-10T20:44:58.870072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1: [1, 1, 2, 3]\n",
      "rdd2: [1, 1, 2, 5]\n",
      "rdd1 substracts rdd2: [3]\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "rdd2=sc.parallelize([1,1,2,5])\n",
    "print('rdd1:', rdd1.collect())\n",
    "print('rdd2:', rdd2.collect())\n",
    "print('rdd1 substracts rdd2:', rdd1.subtract(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:44:59.534265Z",
     "start_time": "2018-08-10T20:44:59.399866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1: [1, 1, 2, 3]\n",
      "rdd2: ['a', 'b']\n",
      "rdd1 cartesian rdd2: [(1, 'a'), (1, 'b'), (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian\n",
    "rdd2=sc.parallelize(['a','b'])\n",
    "print('rdd1:', rdd1.collect())\n",
    "print('rdd2:', rdd2.collect())\n",
    "print('rdd1 cartesian rdd2:', rdd1.cartesian(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count\n",
    "\n",
    "Counting the number of occurances of words in a text is a popular first exercise using map-reduce\n",
    "\n",
    "The task: the input consists of words separated by spaces, the output is a list of words and their counts sorted. We will use the Mobi Dick book as our input.\n",
    "\n",
    "**Define an RDD that will read the file**\n",
    "\n",
    "Execution of read is lazy, file has been opened, reading starts when stage is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:51:02.675082Z",
     "start_time": "2018-08-10T20:51:02.398355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 ms, sys: 0 ns, total: 2.94 ms\n",
      "Wall time: 273 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file=sc.textFile('Data/mobydick.txt')\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps for counting the words**\n",
    "\n",
    "split lines by spaces, map `word` to `(word,1)`, count the number of occurances of each words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:58:23.491242Z",
     "start_time": "2018-08-10T20:58:23.456857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 727 µs, sys: 11.1 ms, total: 11.9 ms\n",
      "Wall time: 25.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words=text_file.flatMap(lambda line: line.split(' '))\n",
    "not_empty=words.filter(lambda x: x != '')\n",
    "key_values=not_empty.map(lambda word: (word, 1))\n",
    "counts=key_values.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T20:58:43.824281Z",
     "start_time": "2018-08-10T20:58:43.486495Z"
    }
   },
   "source": [
    "**The execution plan**\n",
    "\n",
    "In the last cell we defined the execution plan, but we haven't started to execute it. Preparing the plan took ~100ms (which is non-trivial), but much less than the time to execute it. Let's have a look at the execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:03:45.313372Z",
     "start_time": "2018-08-10T21:03:45.305211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[100] at RDD at PythonRDD.scala:48 []\n",
      " |  MapPartitionsRDD[99] at mapPartitions at PythonRDD.scala:122 []\n",
      " |  ShuffledRDD[98] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[97] at reduceByKey at <timed exec>:4 []\n",
      "    |  PythonRDD[96] at reduceByKey at <timed exec>:4 []\n",
      "    |  Data/mobydick.txt MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  Data/mobydick.txt HadoopRDD[84] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(counts.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution**\n",
    "\n",
    "Now the lazy execution model finally performs some actual work, which takes a significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:11:22.863343Z",
     "start_time": "2018-08-10T21:11:22.740606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=19840, total words=115314, mean number of occurances per word=5.81\n",
      "CPU times: user 16 ms, sys: 3.67 ms, total: 19.7 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Count=counts.count() # Count = the number of different words\n",
    "Sum=counts.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean number of occurances per word=%4.2f' % (Count, Sum, float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding most common words\n",
    "\n",
    "`counts`: RDD with 19840 pairs of the form `(word, count)`\n",
    "\n",
    "Find the 5 most frequent words\n",
    "\n",
    "**Method 1**: `collect` and `sort` on head node\n",
    "\n",
    "**Method 2**: pure Spark, `collect` only at the end\n",
    "\n",
    "**Method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:17:18.869552Z",
     "start_time": "2018-08-10T21:17:18.768254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 ms, sys: 3.87 ms, total: 17.9 ms\n",
      "Wall time: 96.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:19:52.960607Z",
     "start_time": "2018-08-10T21:19:52.946761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words \n",
      "the:\t6611\n",
      "of:\t3460\n",
      "and:\t2969\n",
      "a:\t2466\n",
      "to:\t2339\n",
      "CPU times: user 6.72 ms, sys: 0 ns, total: 6.72 ms\n",
      "Wall time: 6.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C.sort(key=lambda x: x[1])\n",
    "print('most common words \\n'+'\\n'.join(['%s:\\t%d' % c for c in reversed(C[-5:])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:24:59.975308Z",
     "start_time": "2018-08-10T21:24:59.965513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 µs, sys: 4 µs, total: 45 µs\n",
      "Wall time: 49.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# step 1: split, clean and map\n",
    "\n",
    "word_pairs=text_file.flatMap(lambda x: x.split(' '))\\\n",
    "                    .filter(lambda x: x != '')\\\n",
    "                    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:25:41.261827Z",
     "start_time": "2018-08-10T21:25:41.224828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 303 µs, total: 15.9 ms\n",
      "Wall time: 29.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# step 2: count occurances of each word\n",
    "\n",
    "counts=word_pairs.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:28:01.959048Z",
     "start_time": "2018-08-10T21:28:01.510747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 ms, sys: 967 µs, total: 31.9 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# step 3: reverse (word, count) to (count, word) and sort by key\n",
    "\n",
    "reverse_counts=counts.map(lambda x: (x[1], x[0]))\n",
    "sorted_counts=reverse_counts.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**full execution plan**\n",
    "\n",
    "But we have to execute it! Not even a single byte has been read from Moby Dick's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:29:24.353937Z",
     "start_time": "2018-08-10T21:29:24.344175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[112] at RDD at PythonRDD.scala:48 []\n",
      " |  MapPartitionsRDD[111] at mapPartitions at PythonRDD.scala:122 []\n",
      " |  ShuffledRDD[110] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[109] at reduceByKey at <timed exec>:3 []\n",
      "    |  PythonRDD[108] at reduceByKey at <timed exec>:3 []\n",
      "    |  Data/mobydick.txt MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  Data/mobydick.txt HadoopRDD[84] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(counts.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T21:30:51.452278Z",
     "start_time": "2018-08-10T21:30:51.218217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      "6611:\tthe\n",
      "3460:\tof\n",
      "2969:\tand\n",
      "2466:\ta\n",
      "2339:\tto\n",
      "CPU times: user 11.3 ms, sys: 289 µs, total: 11.6 ms\n",
      "Wall time: 228 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "D=sorted_counts.take(5)\n",
    "print('most common words\\n'+'\\n'.join(['%d:\\t%s' % c for c in D ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
