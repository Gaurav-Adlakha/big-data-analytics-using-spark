{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark notebook basics\n",
    "\n",
    "**SparkContext**: our way of comunicating to the Spark system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:33.472774Z",
     "start_time": "2018-07-05T00:32:33.467547Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:36.717973Z",
     "start_time": "2018-07-05T00:32:33.475717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master='local[4]')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `master='local[4]` runs spark locally in my notebook using 4 workers (since I have 4 cores, we have one worker per core)\n",
    " \n",
    " We must have only one SparkContext at a time. It is designed for a single user. Before running a new SparkContext, stop the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:36.726379Z",
     "start_time": "2018-07-05T00:32:36.721442Z"
    }
   },
   "outputs": [],
   "source": [
    "# sc.stop() # stop current SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDDs** (Resilient Distributed Dataset): a list of elements stored in several computers\n",
    "\n",
    "**Parallelize**: simplest wat of creating an RDD. It is of type `PythonRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:37.250158Z",
     "start_time": "2018-07-05T00:32:36.730121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize(range(3))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect**: RDD content is distributed among all executors. `collect()` is the inverse of `parallelize()`. Collects the elements of the RDD, returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:38.837817Z",
     "start_time": "2018-07-05T00:32:37.253783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "L=A.collect()\n",
    "print(type(L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using collect eleminates the benefits of parallelism!!\n",
    "\n",
    "It is often tempting to `.collect()` an RDD to make it into a list, and then process it using standard Python. However, this means only the head node is performing the computation, thus not benefitting from Spark. Using RDD operations will make you use all the computers at your disposal\n",
    "\n",
    "**Map**: applies an operation to each element of the RDD. Parameter is the function defining the operation. Returns a new RDD. Operation is performed in parallel on all execution. Each executor operates on the local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.043396Z",
     "start_time": "2018-07-05T00:32:38.845346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce**: takes an RDD, return a single value. Reduce operator takes two elements as input and returns one as output. Repeatedly applies a reduce operator. Each executor reduces the data local to it. The results from all executors are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.197193Z",
     "start_time": "2018-07-05T00:32:39.048637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.383583Z",
     "start_time": "2018-07-05T00:32:39.200347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finds the shortest string\n",
    "words=['this','is','the','best','thinkpad','ever!']\n",
    "wordsRDD=sc.parallelize(words)\n",
    "wordsRDD.reduce(lambda w,v: w if len(w)<len(v) else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.561944Z",
     "start_time": "2018-07-05T00:32:39.389751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad reduce operation\n",
    "B=sc.parallelize([1,3,5,2])\n",
    "B.reduce(lambda x,y: x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't an operation in which the order doesn't matter, because $x-y$ is different from $y-x$. Which of the following did you do:\n",
    "\n",
    "$$((1-3)-5)-2$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(1-3)-(5-2)$$\n",
    "\n",
    "\n",
    "Using regular functions instead of lambda functions: lambda functions are short and sweet, but sometimes it is hard to use it in one line, we can use a full-fledged functions instead\n",
    "\n",
    "Suppose we want to find the last word in a lexigographical order among the longest words in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.581234Z",
     "start_time": "2018-07-05T00:32:39.565228Z"
    }
   },
   "outputs": [],
   "source": [
    "def largerThan(x,y):\n",
    "    if len(x)>len(y):\n",
    "        return x\n",
    "    elif len(y)>len(x):\n",
    "        return y\n",
    "    else:\n",
    "        if x>y:\n",
    "            return x\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:32:39.772434Z",
     "start_time": "2018-07-05T00:32:39.583772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thinkpad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing number of workers\n",
    "\n",
    "**Effect of changing the number of workers**: when you initialize SparkContext, you can specify the number of workers, usually one worker per core, but it can be smaller or larger than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.275899Z",
     "start_time": "2018-07-05T00:32:39.779513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 executors, time=2.352\n",
      " 2 executors, time=1.912\n",
      " 3 executors, time=1.964\n",
      " 4 executors, time=1.792\n",
      " 5 executors, time=1.895\n",
      " 6 executors, time=2.801\n",
      " 7 executors, time=3.518\n",
      " 8 executors, time=3.280\n",
      " 9 executors, time=2.872\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc.stop()\n",
    "for j in range(1,10):\n",
    "    sc = SparkContext(master='local[%d]'%(j))\n",
    "    tic=time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2]*100000).reduce(lambda x,y:x+y)\n",
    "    print('%2d executors, time=%4.3f'%(j,time()-tic))\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution plans lazy eval and caching\n",
    "\n",
    "Suppose our task is $\\sum^n_{i=1}x^2_i$. The standard (busy) way to do this is: i) calculate the square of each element; ii) sum the squares. This requires storing all intermediate results!\n",
    "\n",
    "**Lazy evaluation**: postpone computing the square until result is needed, no need to intermediate results, scan through the data once, rather than twice. We create an RDD with one million elements to amplify the effects of lazy evaluation and caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.365707Z",
     "start_time": "2018-07-05T00:33:13.282561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 206 µs, total: 14.7 ms\n",
      "Wall time: 75.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc=SparkContext()\n",
    "RDD=sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a computation: the role of the function `taketime` is to consume CPU cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.375469Z",
     "start_time": "2018-07-05T00:33:13.369025Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import cos\n",
    "def taketime(i):\n",
    "    [cos(j) for j in range(100)]\n",
    "    return cos(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.394801Z",
     "start_time": "2018-07-05T00:33:13.379566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 µs, sys: 0 ns, total: 55 µs\n",
      "Wall time: 61.3 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5403023058681398"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taketime(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time units\n",
    "* 1 second = 1000 millisecond ($ms$)\n",
    "* 1 millisecond = 1000 microsecond ($\\mu s$)\n",
    "* 1 microsecond = 1000 nanosecond ($\\eta s$)\n",
    "\n",
    "Clock rate: one cycle of a 3GHz CPU takes $\\frac{1}{3} \\eta s$\n",
    "\n",
    "`taketime(1000)` takes about $25\\eta s=75,000$ clock cycles\n",
    "\n",
    "**Map operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.410825Z",
     "start_time": "2018-07-05T00:33:13.401378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 µs, sys: 0 ns, total: 41 µs\n",
      "Wall time: 48.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How come so fast??\n",
    "\n",
    "We expected this map operation to take $1000000\\times 25\\eta s=25 seconds$\n",
    "\n",
    "Why did the previous took ~30 seconds??\n",
    "\n",
    "Because no computation was done! The cell defined an execution plan, but did not execute it yet\n",
    "\n",
    "**Execution plans**: at this point the variable `Interm` doesn't point to an actual data structure. Instead, it points to an execution plan expressed as a **dependence graph**. The dependence graph defines how the RDDs are computed from each other. The dependenec graph associated with an RDD can be printed out using the method `toDebugString()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:33:13.450882Z",
     "start_time": "2018-07-05T00:33:13.415469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[1] at RDD at PythonRDD.scala:48 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 []\n"
     ]
    }
   ],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interm** (4) PythonRDD[1] at RDD at PythonRDD.scala:48 [], the [4] corresponds to the number of partitions\n",
    "\n",
    "**RDD** ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175\n",
    "\n",
    "So, at this point, only the left blocks of the plan have been declared (the parallelize and the map), but not the reduce\n",
    "\n",
    "**Actual execution**: the `reduce` command needs to output an actual output, Spark, therefore has to actually execute the `map` and the `reduce`. Some real computation needs to be done, which takes about 1-3 seconds depending on the machine used and on it's load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:54:08.472818Z",
     "start_time": "2018-07-05T00:53:58.061419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 9 ms, sys: 4.03 ms, total: 13 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How come so fast?**\n",
    "\n",
    "We expected this map operation to take 25 seconds, map+reduce take only ~10 seconds, why?\n",
    "\n",
    "Because we have 4 workers rather than one, because the measurement of a single iteration of `taketime` is an overestimate\n",
    "\n",
    "**Executing a different calculation based on the same plan**: the plan defined by `Interm` might need to be executed more than once. For example, compute the number of map outputs that are larger than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T00:58:51.613968Z",
     "start_time": "2018-07-05T00:58:36.824358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 15.5 ms, sys: 0 ns, total: 15.5 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The price of not materializing**: the runtime is similar of that of the reduce, because the intermediate results in `Interm` have not been saved in memory (materialized), they need to be recomputed\n",
    "\n",
    "The middle block is executed twice, once for each final step\n",
    "\n",
    "    RDD                     Interm\n",
    "parallelize(range(1000000)) -> map(taketime()) -> reduce(lambda x,y: x+y)       -> number\n",
    "\n",
    "                                                             \\\n",
    "                                                             filter(lambda x: x>0).count()   -> number\n",
    "                                                             \n",
    "**Caching intermediate results**: we sometimes want to keep the intermediate results in memory so that we can reuse them later without recalculating. This will reduce the running time, at the cost of requiring more memory.\n",
    "\n",
    "The method `cache()` indicates that the RDD generated in this plan should be stored in memory. Note that this is a **plan to cache**. The actual caching will be done only when the final result is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T01:07:33.681990Z",
     "start_time": "2018-07-05T01:07:33.652238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.12 ms, sys: 487 µs, total: 9.6 ms\n",
      "Wall time: 24.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Interm=RDD.map(lambda x: taketime(x)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan to cache**: the definition of `Interm` is almost the same as before. However, the plan corresponding to `Interm` is more elaborate and contains information about how the intermediate results will be cached nad replicated. Note that `PythonRDD[4]` is now `Memory Serialized 1x Replicated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T01:10:39.063306Z",
     "start_time": "2018-07-05T01:10:39.045502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4) PythonRDD[7] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "print(Interm.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the cache**: the following command executes the first map-reduce command **and** caches the result of the `map` command in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T01:12:33.067166Z",
     "start_time": "2018-07-05T01:12:32.687706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= -0.2887054679684464\n",
      "CPU times: user 12.7 ms, sys: 38 µs, total: 12.7 ms\n",
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the cache**: this time `Interm` is cached. Therefore the second use of `Interm` is much faster than when we did'nt use cache: 0.35 seconds instead of 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-05T01:14:23.398695Z",
     "start_time": "2018-07-05T01:14:23.032154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= 500000\n",
      "CPU times: user 7.08 ms, sys: 8.08 ms, total: 15.2 ms\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('out=',Interm.filter(lambda x: x>0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
