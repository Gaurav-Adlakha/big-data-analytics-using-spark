{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map reduce\n",
    "\n",
    "**Map reduce** is a programming pattern that is used a lot in big distributed data computation\n",
    "\n",
    "**Map**: square each item in a list $L=[0,1,2,3]$, output is $[0,1,4,9]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.722310Z",
     "start_time": "2018-07-01T17:38:31.683408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## for loop\n",
    "L = [0, 1, 2, 3]\n",
    "O = []\n",
    "for i in L:\n",
    "    O.append(i*i)\n",
    "    \n",
    "## list comprehension\n",
    "[i*i for i in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.736052Z",
     "start_time": "2018-07-01T17:38:31.726560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "\n",
    "list(map(lambda x: x*x, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"traditional\" way computes from first to last in order whereas in the map-reduce strategy the computation order is not specified\n",
    "\n",
    "**Reduce**: compute the sum of a list $L=[3,1,5,7]$, output is $16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.757292Z",
     "start_time": "2018-07-01T17:38:31.739930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## use builtin\n",
    "L = [3, 1, 5, 7]\n",
    "sum(L)\n",
    "\n",
    "## for loop\n",
    "s = 0\n",
    "for i in L:\n",
    "    s += i\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.772198Z",
     "start_time": "2018-07-01T17:38:31.762356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda x, y: x + y, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional way computes everything from first to last in order whereas in the map-reduce strategy the computation order is not specified\n",
    "\n",
    "**Map + Reduce**: compute the sum of squares from a list $L=[0,1,2,3]$, note the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.790516Z",
     "start_time": "2018-07-01T17:38:31.777136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## for loop\n",
    "L = [0, 1, 2, 3]\n",
    "s = 0\n",
    "for i in L:\n",
    "    s += i*i\n",
    "    \n",
    "## list comprehension\n",
    "sum([i*i for i in L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.813334Z",
     "start_time": "2018-07-01T17:38:31.794541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map-reduce\n",
    "\n",
    "reduce(lambda x, y: x + y, map(lambda i: i*i, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional way computes everything from first to last order and we are basically describing exactly what should happen, thinking about the computer being in one command at a time whereas the map-reduce strategy the computation order is not specified and we specify an execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:31.826458Z",
     "start_time": "2018-07-01T17:38:31.817484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the WRONG way \n",
    "reduce(lambda x, y: x+y * y, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-reduce operations should not depend on order of items in the list (commutativity) and order of operations (associativity)\n",
    "\n",
    "**Order of independence**: the result of map or reduce does not depend on the order. The computation order can be chosen by the compiler/optimizer. It allows for parallel computation of sums of subsets. Modern hardware calls for parallel computation but parallel computation is very hard to program\n",
    "\n",
    "Map-reduce is the basis for many systems and for big data, Hadoop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short history of map-reduce\n",
    "\n",
    "**Google File System (GFS) + Map-reduce (2003)**\n",
    "\n",
    "In 2003, Google had a lot of computers, but each was its own independent computer. So, they designed a system called HD, in which there is a master that basically knows where all the data is and the data itself is distributed across a lot of computers. A large file is choped into smaller pieces, and each piece is replicated across two or three computers. So now, we could process things in parallel. Each computer can do map operations on the pieces of data it has and it can start doing reduce operations, it only communicates the final answer to other computers once it finishes its own reduce.\n",
    "\n",
    "**Apache Hadoop (2006)**\n",
    "\n",
    "An open-source implementation of Google's idea, the file system is called Hadoop File System (HDFS), the compute system was called Google MapReduce, in Apache is Hadoop MapReduce. Large eco-system: Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Cloudera Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm\n",
    "\n",
    "**Apache Spark (2014)**\n",
    "\n",
    "Matei Zaharia, MPLab, Berkeley. Main difference from Hadoop: distributed memory instead of distributed files!\n",
    "\n",
    "The native language of the Hadoop eco-system is Java. Spark can be programmed in Java, but code tends to be long. **Scala** (built on top of Java) allows for parallel programming to be abstracted. It is the core language for Spark, but one of the problems is its small user base (you will want to learn Scala if you want to extend Spark). **Pyspark** is a Python library for programming Spark, it is not the most efficient, but it is easier to learn.\n",
    "\n",
    "**Spark Architecture: SC and RDD**\n",
    "\n",
    "SparkContext: control of other nodes is achieved through a special object called the **SparkContext** (usually named **sc**). A notebook can have only one SparkContext object. Initialization is usually `sc = SparkContext()`, use parameters for non-default configuration\n",
    "\n",
    "Resilient Distributed Dataset (RDD): it is a list whose elements are distributed over several computers. The main data structure in Spark. When in RDD form, the elements of the list can be manipulated only through RDD specific methods. RDDs are created from a list on the master node or from a file. RDDs can be translated back to a local list using `collect()`\n",
    "\n",
    "**Pyspark**: some basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:32.262596Z",
     "start_time": "2018-07-01T17:38:31.830563Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:35.129742Z",
     "start_time": "2018-07-01T17:38:32.266496Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:35.534067Z",
     "start_time": "2018-07-01T17:38:35.132639Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize an RDD\n",
    "RDD = sc.parallelize([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.002543Z",
     "start_time": "2018-07-01T17:38:35.537183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the squares of the items\n",
    "RDD.map(lambda x: x*x).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations take a RDD and map it to a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.181984Z",
     "start_time": "2018-07-01T17:38:37.005957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize RDD\n",
    "RDD = sc.parallelize([0,1,2])\n",
    "# sum the squares of the items\n",
    "A = RDD.map(lambda x: x*x)\n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect()` collects all the items in the RDD into a list in the master. If the RDD is large, this can take a long time\n",
    "\n",
    "Checking the start of an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.379979Z",
     "start_time": "2018-07-01T17:38:37.184870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element = 0\n",
      "first 5 elements = [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# initialize a largish RDD\n",
    "n = 10000\n",
    "B = sc.parallelize(range(n))\n",
    "\n",
    "# get the first few elements of an RDD\n",
    "print('first element =', B.first())\n",
    "print('first 5 elements =', B.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.514516Z",
     "start_time": "2018-07-01T17:38:37.382428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045, 2014, 2488, 5180]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "B = sc.parallelize(range(n))\n",
    "\n",
    "# sample about m elements into a new RDD\n",
    "m = 5.\n",
    "C = B.sample(False, m/n)\n",
    "C.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run results in a different sample, sample size varies, expected size is 5, result is an RDD, need to collect to list, sampling is very useful to Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark architecture\n",
    "\n",
    "In local installation, cores serve as master and slaves. In a hardware organization, there is one computer which is the master and other computer which are slaves, the master controls everything, but the slaves do the actual work and store all the data.\n",
    "\n",
    "**Spatial software organization**: the head node (represented by the Spark Driver and the Cluster Master Mesos, YARN or Standalone) and the workers (Cluster worker and the executor), each one has a useful part (Spark Driver and Executor) which is actually doing the work and a management part (Cluster master and Cluster worker).\n",
    "\n",
    "**Spark driver**: the driver runs the master, it is the program you wrote, it executes the `main()` code of your program\n",
    "\n",
    "**Cluster master**: manages computation resources\n",
    "\n",
    "**Worker**: manges a single core. Each RDD is partitioned among the workers. Workers manage partitions and executors. Executors execute tasks on their partition (are myopic)\n",
    "\n",
    "The SparkContext is the abstraction that encapsulates the cluster for the driver node and the programmer. Worker nodes manage resources in a single slave machine. Worker nodes communicates with the cluster manager. Executors are the processes that can perform tasks. Cache refers to the local memory on the slave machine\n",
    "\n",
    "Materialization: we don't always need to store intermediate results. Consider RDD1 -> Map (x: x\\*x) -> RDD2 -> Reduce (x,y: x+y) -> float (in the head node). RDD2 can be consumed as it is being generated, it doesn't have to be materialized (stored in memory). RDDs in general by default are not materialized.\n",
    "\n",
    "**Temporal organization**: The **stage** is a set of operations that can be done before a materialization is necessary. After a set of operations, a stage ends when the RDD needs to be materialized\n",
    "\n",
    "Terms and concepts:\n",
    "\n",
    "* RDDs are partitioned across workers\n",
    "* RDD graph defines the lineage of the RDDs\n",
    "* SparkContext divides the RDD graph into stages which define the execution plan (or physical plan)\n",
    "* A task corresponds to one stage restricted to one partition\n",
    "* An executor is a process that performs tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating plain RDDs\n",
    "\n",
    "Plain RDDs are parallelized lists of elements. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.549654Z",
     "start_time": "2018-07-01T17:38:37.518119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.570402Z",
     "start_time": "2018-07-01T17:38:37.553067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[11] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['you are my sunshine',\n",
    "                'my only sunshine',\n",
    "                'you make me happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three groups of commands**\n",
    "\n",
    "* Creation: RDD from files, databases, or data on driver node\n",
    "* Transformations: RDD to RDD\n",
    "* RDD to data on driver node, databases, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.702103Z",
     "start_time": "2018-07-01T17:38:37.574160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(4)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.833537Z",
     "start_time": "2018-07-01T17:38:37.704844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(4)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.937789Z",
     "start_time": "2018-07-01T17:38:37.835891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize(range(4))\n",
    "A.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating KeyVal RDD\n",
    "\n",
    "Each element of the RDD is a pair (key,value). Key is an identifier and the value can be anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.953279Z",
     "start_time": "2018-07-01T17:38:37.941448Z"
    }
   },
   "outputs": [],
   "source": [
    "database = sc.parallelize((55632, {'name': 'yoav', 'city': 'jerusalem'},\n",
    "                           33421, {'name': 'homer', 'city': 'fairview'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:37.968565Z",
     "start_time": "2018-07-01T17:38:37.957287Z"
    }
   },
   "outputs": [],
   "source": [
    "car_count = sc.parallelize((('honda',3),\n",
    "                            ('subaru',2),\n",
    "                            ('honda',1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations in RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:38.109181Z",
     "start_time": "2018-07-01T17:38:37.972050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize(range(4))\\\n",
    "    .map(lambda x: (x, x*x))\n",
    "    \n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReduceByKey**: perform reduce separately on each key value. Note transformation, not action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:38.895555Z",
     "start_time": "2018-07-01T17:38:38.116337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 100), (1, -15), (3, 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize([(1,3),(4,100),(1,-5),(3,2)])\n",
    "A.reduceByKey(lambda x,y: x*y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys are 1, 4, 1 and 3. The keys 4 and 3 are unique, only 1 repeats, so the operation will multiply $3\\times-5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first loop, first the temporary range list is created, then loop iterates over it.\n",
    "\n",
    "In the second loop, the elements of the temporary list are created and \"destroyed\" as needed, so no memory is wasted\n",
    "\n",
    "```\n",
    "# waster of memory space:\n",
    "for i in range(1000000):\n",
    "    # do something\n",
    "    \n",
    "# no waste\n",
    "for i in xrange(1000000):\n",
    "    # do something\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupByKey**: returns a `(key,<iterator>)` pair for each key value. The iterator iterator over the values corresponding to the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:39.204210Z",
     "start_time": "2018-07-01T17:38:38.900085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [3, -5]), (3, [100, 2])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize([(1,3),(3,100),(1,-5),(3,2)])\n",
    "# A.groupByKey().map(lambda k,value: (k, [val for val in value])).collect()\n",
    "A.groupByKey().map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions in RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countByKey** returns a python dictionary with the number of pairs for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:39.405755Z",
     "start_time": "2018-07-01T17:38:39.217786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 3: 2})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize([(1,3),(3,100),(1,-5),(3,2)])\n",
    "A.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lookup(key)**: returns the list of all of the values associated with the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:39.587882Z",
     "start_time": "2018-07-01T17:38:39.409674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize([(1,3),(3,100),(1,-5),(3,2)])\n",
    "A.lookup(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collectAsMap()**: like collect() but instead of returning a list of tuples it returns a Map (it is a Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T17:38:39.677235Z",
     "start_time": "2018-07-01T17:38:39.590993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -5, 3: 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize([(1,3),(3,100),(1,-5),(3,2)])\n",
    "A.collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
