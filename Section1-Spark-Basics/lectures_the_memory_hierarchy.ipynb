{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "It is becoming impossible to store data in a single computer, that's when parallel data analysis comes into play. Yoav Freund from UCSD. We will use Apache Spark with _pyspark_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory latency\n",
    "\n",
    "We will learn how to use computers' clusters to efficiently process large amounts of data.\n",
    "\n",
    "Before, let's understand the problem, what makes computation on very large datasets very slow?\n",
    "\n",
    "At a high level, any computer has two main parts: CPU (central processing unit) and storage. If we need to multiply two numbers, initially the numbers are in storage; at the first step, the numbers are read from storage and into registers A and B in the CPU, then the numbers are multiplied and the result is stored in a third register C, finnaly C is written back to storage.\n",
    "\n",
    "The time to complete one step is called step latency, and to complete all stel total latency:\n",
    "\n",
    "1. Read A\n",
    "2. Read B\n",
    "3. C = A * B\n",
    "4. Write C\n",
    "\n",
    "With big data, most of the latency is memory latency (1,2,4), not computation (3)\n",
    "\n",
    "Storage latency varies widely: main memory (RAM) is fast, while spinning memory is slow, memory that resides in a different computer depends on the load of the network\n",
    "\n",
    "Big data analytics revolves around methods for organizing storage and computation in ways that maximize speed while minimizing cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache\n",
    "\n",
    "Latency, size and price: we need to trade-off speed and storage size.\n",
    "\n",
    "Caching is a way for combining fast and slow memory, to create storage that is both fast and large.\n",
    "\n",
    "Consider a computer with a CPU, a fast and small cache (8 bytes) and a slow and large memory (80 bytes). The CPU first check the cache for the content, if it is, it can be retrieved quickly (cache hit), if not, then retrieval will be slow (cache miss).\n",
    "\n",
    "The cache is effective is the hit rate is high.\n",
    "\n",
    "Temporal locality: multiple accesses to same address within a shift time period\n",
    "\n",
    "Spatial locality: multiple accesses to close-together addresses in short time period (eg. difference between two sums, counting words by sorting). The cache benefit from locality because memory is partitioned into blocks/lines rather than single bytes, moving a block of memory takes much less time than moving each byte individually. Memory locations that are close to each other are likely to fall in the same block, resulting in more cache hits.\n",
    "\n",
    "How sorting improves the locality of word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory access locality\n",
    "\n",
    "Access locality refers to the ability of software to make good use of the cache\n",
    "\n",
    "Memory is broken into pages. Software that uses the same of neighboring pages repeatedly has good access locality. \n",
    "Hardware is designed to speed up such software\n",
    "\n",
    "* Temporal locality: accessing the same element again and again\n",
    "\n",
    "Suppose the task is compute a function - $f_\\theta(x)$ - over a long sequence $x_1,x_2,...,x_n$, $\\theta$ is a parameters vector (eg. weights of a neural network), the parameters $\\theta$ are needed for each computation, if $\\theta$ fits in the cache, access is fast. If $\\theta$ does not fit the cache, each $x_i$ causes at least two cache misses\n",
    "\n",
    "* Spatial locality: you don't access the same element, but the next element, and the next element etc.\n",
    "\n",
    "Suppose the task is compute the function $\\sum^{n-1}_{i=1}(x_i-x_{i+1})^2$ on $x_1,x_2,...,x_n$. Contrast two ways of storing it: i) a linked list (poor locality); ii) indexed array (good locality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
