{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map reduce\n",
    "\n",
    "**Map reduce** is a programming pattern that is used a lot in big distributed data computation\n",
    "\n",
    "**Map**: square each item in a list $L=[0,1,2,3]$, output is $[0,1,4,9]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T22:47:21.205638Z",
     "start_time": "2018-06-27T22:47:21.181312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## for loop\n",
    "L = [0, 1, 2, 3]\n",
    "O = []\n",
    "for i in L:\n",
    "    O.append(i*i)\n",
    "    \n",
    "## list comprehension\n",
    "[i*i for i in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T22:48:34.272831Z",
     "start_time": "2018-06-27T22:48:34.265825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "\n",
    "list(map(lambda x: x*x, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"traditional\" way computes from first to last in order whereas in the map-reduce strategy the computation order is not specified\n",
    "\n",
    "**Reduce**: compute the sum of a list $L=[3,1,5,7]$, output is $16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T22:56:42.049824Z",
     "start_time": "2018-06-27T22:56:42.031016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## use builtin\n",
    "L = [3, 1, 5, 7]\n",
    "sum(L)\n",
    "\n",
    "## for loop\n",
    "s = 0\n",
    "for i in L:\n",
    "    s += i\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T22:56:48.611308Z",
     "start_time": "2018-06-27T22:56:48.594487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda x, y: x + y, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional way computes everything from first to last in order whereas in the map-reduce strategy the computation order is not specified\n",
    "\n",
    "**Map + Reduce**: compute the sum of squares from a list $L=[0,1,2,3]$, note the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T23:01:34.877581Z",
     "start_time": "2018-06-27T23:01:34.864365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional way\n",
    "\n",
    "## for loop\n",
    "L = [0, 1, 2, 3]\n",
    "s = 0\n",
    "for i in L:\n",
    "    s += i*i\n",
    "    \n",
    "## list comprehension\n",
    "sum([i*i for i in L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T23:02:42.093487Z",
     "start_time": "2018-06-27T23:02:42.083685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map-reduce\n",
    "\n",
    "reduce(lambda x, y: x + y, map(lambda i: i*i, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional way computes everything from first to last order and we are basically describing exactly what should happen, thinking about the computer being in one command at a time whereas the map-reduce strategy the computation order is not specified and we specify an execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T23:06:23.321636Z",
     "start_time": "2018-06-27T23:06:23.314790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the WRONG way \n",
    "reduce(lambda x, y: x+y * y, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-reduce operations should not depend on order of items in the list (commutativity) and order of operations (associativity)\n",
    "\n",
    "**Order of independence**: the result of map or reduce does not depend on the order. The computation order can be chosen by the compiler/optimizer. It allows for parallel computation of sums of subsets. Modern hardware calls for parallel computation but parallel computation is very hard to program\n",
    "\n",
    "Map-reduce is the basis for many systems and for big data, Hadoop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short history of map-reduce\n",
    "\n",
    "**Google File System (GFS) + Map-reduce (2003)**\n",
    "\n",
    "In 2003, Google had a lot of computers, but each was its own independent computer. So, they designed a system called HD, in which there is a master that basically knows where all the data is and the data itself is distributed across a lot of computers. A large file is choped into smaller pieces, and each piece is replicated across two or three computers. So now, we could process things in parallel. Each computer can do map operations on the pieces of data it has and it can start doing reduce operations, it only communicates the final answer to other computers once it finishes its own reduce.\n",
    "\n",
    "**Apache Hadoop (2006)**\n",
    "\n",
    "An open-source implementation of Google's idea, the file system is called Hadoop File System (HDFS), the compute system was called Google MapReduce, in Apache is Hadoop MapReduce. Large eco-system: Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache Zookeeper, Cloudera Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm\n",
    "\n",
    "**Apache Spark (2014)**\n",
    "\n",
    "Matei Zaharia, MPLab, Berkeley. Main difference from Hadoop: distributed memory instead of distributed files!\n",
    "\n",
    "The native language of the Hadoop eco-system is Java. Spark can be programmed in Java, but code tends to be long. **Scala** (built on top of Java) allows for parallel programming to be abstracted. It is the core language for Spark, but one of the problems is its small user base (you will want to learn Scala if you want to extend Spark). **Pyspark** is a Python library for programming Spark, it is not the most efficient, but it is easier to learn.\n",
    "\n",
    "**Spark Architecture: SC and RDD**\n",
    "\n",
    "SparkContext: control of other nodes is achieved through a special object called the **SparkContext** (usually named **sc**). A notebook can have only one SparkContext object. Initialization is usually `sc = SparkContext()`, use parameters for non-default configuration\n",
    "\n",
    "Resilient Distributed Dataset (RDD): it is a list whose elements are distributed over several computers. The main data structure in Spark. When in RDD form, the elements of the list can be manipulated only through RDD specific methods. RDDs are created from a list on the master node or from a file. RDDs can be translated back to a local list using `collect()`\n",
    "\n",
    "**Pyspark**: some basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:35:51.932935Z",
     "start_time": "2018-06-29T02:35:51.704076Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:36:05.619678Z",
     "start_time": "2018-06-29T02:36:02.172749Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:36:29.303886Z",
     "start_time": "2018-06-29T02:36:29.044687Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize an RDD\n",
    "RDD = sc.parallelize([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:37:23.544354Z",
     "start_time": "2018-06-29T02:37:21.798626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the squares of the items\n",
    "RDD.map(lambda x: x*x).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations take a RDD and map it to a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:41:19.308016Z",
     "start_time": "2018-06-29T02:41:19.173469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize RDD\n",
    "RDD = sc.parallelize([0,1,2])\n",
    "# sum the squares of the items\n",
    "A = RDD.map(lambda x: x*x)\n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect()` collects all the items in the RDD into a list in the master. If the RDD is large, this can take a long time\n",
    "\n",
    "Checking the start of an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:45:31.372770Z",
     "start_time": "2018-06-29T02:45:31.268166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element = 0\n",
      "first 5 elements = [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# initialize a largish RDD\n",
    "n = 10000\n",
    "B = sc.parallelize(range(n))\n",
    "\n",
    "# get the first few elements of an RDD\n",
    "print('first element =', B.first())\n",
    "print('first 5 elements =', B.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T02:47:52.382010Z",
     "start_time": "2018-06-29T02:47:52.301377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5518, 7239]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "B = sc.parallelize(range(n))\n",
    "\n",
    "# sample about m elements into a new RDD\n",
    "m = 5.\n",
    "C = B.sample(False, m/n)\n",
    "C.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run results in a different sample, sample size varies, expected size is 5, result is an RDD, need to collect to list, sampling is very useful to Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
