{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "It is becoming impossible to store data in a single computer, that's when parallel data analysis comes into play. Yoav Freund from UCSD. We will use Apache Spark with _pyspark_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory latency\n",
    "\n",
    "We will learn how to use computers' clusters to efficiently process large amounts of data.\n",
    "\n",
    "Before, let's understand the problem, what makes computation on very large datasets very slow?\n",
    "\n",
    "At a high level, any computer has two main parts: CPU (central processing unit) and storage. If we need to multiply two numbers, initially the numbers are in storage; at the first step, the numbers are read from storage and into registers A and B in the CPU, then the numbers are multiplied and the result is stored in a third register C, finnaly C is written back to storage.\n",
    "\n",
    "The time to complete one step is called step latency, and to complete all stel total latency:\n",
    "\n",
    "1. Read A\n",
    "2. Read B\n",
    "3. C = A * B\n",
    "4. Write C\n",
    "\n",
    "With big data, most of the latency is memory latency (1,2,4), not computation (3)\n",
    "\n",
    "Storage latency varies widely: main memory (RAM) is fast, while spinning memory is slow, memory that resides in a different computer depends on the load of the network\n",
    "\n",
    "Big data analytics revolves around methods for organizing storage and computation in ways that maximize speed while minimizing cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache\n",
    "\n",
    "Latency, size and price: we need to trade-off speed and storage size.\n",
    "\n",
    "Caching is a way for combining fast and slow memory, to create storage that is both fast and large.\n",
    "\n",
    "Consider a computer with a CPU, a fast and small cache (8 bytes) and a slow and large memory (80 bytes). The CPU first check the cache for the content, if it is, it can be retrieved quickly (cache hit), if not, then retrieval will be slow (cache miss).\n",
    "\n",
    "The cache is effective is the hit rate is high.\n",
    "\n",
    "Temporal locality: multiple accesses to same address within a shift time period\n",
    "\n",
    "Spatial locality: multiple accesses to close-together addresses in short time period (eg. difference between two sums, counting words by sorting). The cache benefit from locality because memory is partitioned into blocks/lines rather than single bytes, moving a block of memory takes much less time than moving each byte individually. Memory locations that are close to each other are likely to fall in the same block, resulting in more cache hits.\n",
    "\n",
    "How sorting improves the locality of word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory access locality\n",
    "\n",
    "Access locality refers to the ability of software to make good use of the cache\n",
    "\n",
    "Memory is broken into pages. Software that uses the same of neighboring pages repeatedly has good access locality. \n",
    "Hardware is designed to speed up such software\n",
    "\n",
    "* Temporal locality: accessing the same element again and again\n",
    "\n",
    "Suppose the task is compute a function - $f_\\theta(x)$ - over a long sequence $x_1,x_2,...,x_n$, $\\theta$ is a parameters vector (eg. weights of a neural network), the parameters $\\theta$ are needed for each computation, if $\\theta$ fits in the cache, access is fast. If $\\theta$ does not fit the cache, each $x_i$ causes at least two cache misses\n",
    "\n",
    "* Spatial locality: you don't access the same element, but the next element, and the next element etc.\n",
    "\n",
    "Suppose the task is compute the function $\\sum^{n-1}_{i=1}(x_i-x_{i+1})^2$ on $x_1,x_2,...,x_n$. Contrast two ways of storing it: i) a linked list (poor locality); ii) indexed array (good locality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row-wise vs column-wise scanning\n",
    "\n",
    "The way you traverse a 2D array effects speed\n",
    "\n",
    "numpy arrays are, by default, organized in a row-major order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T02:52:44.441128Z",
     "start_time": "2018-06-27T02:52:44.326201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T02:52:44.472023Z",
     "start_time": "2018-06-27T02:52:44.447245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       "       [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([range(1, 31)]).reshape([3, 10])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a[i,j]$ and $a[i,j+1]$ are placed in consecutive places in memory\n",
    "\n",
    "$a[i,j]$ and $a[i+1,j]$ are 10 memory locations apart\n",
    "\n",
    "This implies that scanning the array row by row is more local than scanning column by column (locality implies speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T02:52:44.484352Z",
     "start_time": "2018-06-27T02:52:44.475631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 µs, sys: 39 µs, total: 222 µs\n",
      "Wall time: 120 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# scan column by column\n",
    "s=0\n",
    "for i in range(a.shape[1]): s+=sum(a[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T02:52:44.498800Z",
     "start_time": "2018-06-27T02:52:44.488187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 µs, sys: 29 µs, total: 157 µs\n",
      "Wall time: 60.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# scan row by row\n",
    "s=0\n",
    "for i in range(a.shape[0]): s+=sum(a[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T00:53:53.964459Z",
     "start_time": "2018-06-27T00:53:53.954049Z"
    }
   },
   "source": [
    "* Conclusions:\n",
    "\n",
    "Traversing a numpy array column by column takes more time than row by row\n",
    "\n",
    "The effect increases proportionaly to the number of elements in the array (square of one dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring latency\n",
    "\n",
    "Goals: measure the effects of caching in the wild and understand how to study long tail distributions\n",
    "\n",
    "**Latency** is the time difference between the time at which the CPU is issuing a read or write command and the time the command is complete\n",
    "\n",
    "This time is very short if the element is already in the L1 Cache and is very long if the element is in external memory (disk or SSD)\n",
    "\n",
    "We'll test arrays of sizes [zero, 1kb, 1mb, 1gb, 10gb], we'll perform 100,000 read/write ops to random locations in the array, we'll analyze the distribution of the latencies as a function of the size of the array\n",
    "\n",
    "Looking just at the mean and standard deviation of the latencies, we could assume we have a normal distribution, but in fact latency have a much longer tail (higher probability of a high latency) than a Gaussian distribution, which we could observe from the CDF\n",
    "\n",
    "* Characterize sequential access\n",
    "\n",
    "Random access degrades rapidly with the size of the block. Sequential access is much faster. We already saw that writting 10GB to disk sequentially takes 8.9s (less than 1s for 1Gb). Writting a 1TB disk at this rate would take ~16min\n",
    "\n",
    "**Bandwith**: total number of GB that we can write in so many seconds. We are measuring bandwidth rather than latency. We say that it takes 8.9s to write 10GB to SSD, we are not saying that it takes $8.9e^{-10}$ to write a single byte, because many write operations are occuring in parallel\n",
    "\n",
    "Byte-rate for writing large blocks is about 100MB/s to RAM\n",
    "\n",
    "Byte-rate for writing large SSD blocks is about 1GB/s\n",
    "\n",
    "The cost-effective solution is often a cluster of many cheap computers, each with many cores and break up data so that each computer has a small fraction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory hierarchy\n",
    "\n",
    "Cache in combination with a local access pattern leads to speed up in computation\n",
    "\n",
    "Real systems have several levels of storage types: small and fast storage close to CPU are top hierarchy whereas large and slow storage further from CPU are botttom hierarchy, from the CPU registers, L1, L2 and L3 cache, main memory or RAM, disk storage and local area network.\n",
    "\n",
    "**Cluster** is a collection of many computers. A data processing cluster is simply many computers linked through an ethernet connection, storage is shared. **Shuffling** is the mechanism of transferring data between computers. In Spark, the transferring mechanism is called **resilient distribution data set or RDD** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of large scale computing\n",
    "\n",
    "Super computers: Cray, Deep Blue, Blue Gene. Specialized hardware, extremely expensive, created to solve specialized important problems\n",
    "\n",
    "Data centers: physical side of what we call cloud computing. Collection of commodity computers (computers of out daily use), vast number of computers (100,000s), created to provide computation for large and small organizations, computation as a commodity\n",
    "\n",
    "Google 2003: Larry Page and Sergey Brin develop a method for storing very large files on multiple commodity computers, each file is broken into fixed-sized chunks. Each chunk is stored on multiiple chunk servers, the locations of the chunks is managed by the master computer\n",
    "\n",
    "Properties of GFS/HDFS: commodity hardware (low cost per byte of storage), locality (data stored close to CPU), redundancy (can recover from server failures), abstraction (looks to user like standard file system, chunk mechanism is hidden)\n",
    "\n",
    "**HDFS (Hadoop file server)** is a storage abstraction, a way to store files on many machines\n",
    "\n",
    "**Map reduce** is a computation abstraction that works well with HDFS\n",
    "\n",
    "It allows the programmer to specify parallel computation without kowning how the hardware is organized\n",
    "\n",
    "Spark was developed by Matei Zaharia in 2014, Hadoop uses shared file system (disk), Spark uses shared main memory (faster, lower latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
